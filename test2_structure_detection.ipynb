{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from utils.docai_client import get_client_from_env\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Upload PDF File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì§ Upload your PDF file (should have tables/structure):\")\n",
        "pdf_uploaded = files.upload()\n",
        "\n",
        "pdf_filename = list(pdf_uploaded.keys())[0]\n",
        "os.makedirs('sample_pdfs', exist_ok=True)\n",
        "pdf_path = f'sample_pdfs/{pdf_filename}'\n",
        "\n",
        "with open(pdf_path, 'wb') as f:\n",
        "    f.write(pdf_uploaded[pdf_filename])\n",
        "\n",
        "print(f\"‚úÖ PDF saved to: {pdf_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Define Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bounding_box(bounding_poly):\n",
        "    \"\"\"Extract normalized bounding box from polygon\"\"\"\n",
        "    if not bounding_poly or not hasattr(bounding_poly, 'normalized_vertices'):\n",
        "        return {\"x_min\": 0, \"y_min\": 0, \"x_max\": 0, \"y_max\": 0}\n",
        "    \n",
        "    vertices = bounding_poly.normalized_vertices\n",
        "    if not vertices:\n",
        "        return {\"x_min\": 0, \"y_min\": 0, \"x_max\": 0, \"y_max\": 0}\n",
        "    \n",
        "    x_coords = [v.x for v in vertices]\n",
        "    y_coords = [v.y for v in vertices]\n",
        "    \n",
        "    return {\n",
        "        \"x_min\": min(x_coords),\n",
        "        \"y_min\": min(y_coords),\n",
        "        \"x_max\": max(x_coords),\n",
        "        \"y_max\": max(y_coords)\n",
        "    }\n",
        "\n",
        "def extract_table_cells(table, full_text):\n",
        "    \"\"\"Extract table cells into structured format\"\"\"\n",
        "    cells = []\n",
        "    \n",
        "    # Extract header rows\n",
        "    if hasattr(table, 'header_rows'):\n",
        "        for row in table.header_rows:\n",
        "            for cell in row.cells:\n",
        "                text = extract_cell_text(cell, full_text)\n",
        "                cells.append({\n",
        "                    \"row\": cell.layout.table_row_index if hasattr(cell.layout, 'table_row_index') else 0,\n",
        "                    \"col\": cell.layout.table_col_index if hasattr(cell.layout, 'table_col_index') else 0,\n",
        "                    \"text\": text,\n",
        "                    \"is_header\": True\n",
        "                })\n",
        "    \n",
        "    # Extract body rows\n",
        "    if hasattr(table, 'body_rows'):\n",
        "        for row in table.body_rows:\n",
        "            for cell in row.cells:\n",
        "                text = extract_cell_text(cell, full_text)\n",
        "                cells.append({\n",
        "                    \"row\": cell.layout.table_row_index if hasattr(cell.layout, 'table_row_index') else 0,\n",
        "                    \"col\": cell.layout.table_col_index if hasattr(cell.layout, 'table_col_index') else 0,\n",
        "                    \"text\": text,\n",
        "                    \"is_header\": False\n",
        "                })\n",
        "    \n",
        "    return cells\n",
        "\n",
        "def extract_cell_text(cell, full_text):\n",
        "    \"\"\"Extract text from a table cell\"\"\"\n",
        "    if not hasattr(cell, 'layout') or not cell.layout.text_anchor:\n",
        "        return \"\"\n",
        "    \n",
        "    text_segments = []\n",
        "    for segment in cell.layout.text_anchor.text_segments:\n",
        "        text = full_text[segment.start_index:segment.end_index]\n",
        "        text_segments.append(text)\n",
        "    \n",
        "    return \" \".join(text_segments).strip()\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " out what part of plan is impelement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TEST 2: STRUCTURE DETECTION\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# Initialize client\n",
        "print(\"üì° Initializing Document AI client...\")\n",
        "client = get_client_from_env()\n",
        "\n",
        "# Process document\n",
        "print(f\"\\nüìÑ Processing PDF: {pdf_path}\")\n",
        "document = client.process_document(pdf_path)\n",
        "\n",
        "# Extract structured elements\n",
        "print(\"\\nüîç Detecting document structure...\")\n",
        "\n",
        "all_elements = []\n",
        "stats = {\n",
        "    \"paragraphs\": 0,\n",
        "    \"tables\": 0,\n",
        "    \"images\": 0,\n",
        "    \"headers\": 0,\n",
        "    \"footers\": 0\n",
        "}\n",
        "\n",
        "for page_num, page in enumerate(document.pages, 1):\n",
        "    print(f\"  Processing page {page_num}...\")\n",
        "    \n",
        "    # Extract paragraphs\n",
        "    for para_idx, paragraph in enumerate(page.paragraphs):\n",
        "        if paragraph.layout.text_anchor:\n",
        "            # Get text\n",
        "            text_segments = []\n",
        "            for segment in paragraph.layout.text_anchor.text_segments:\n",
        "                text = document.text[segment.start_index:segment.end_index]\n",
        "                text_segments.append(text)\n",
        "            \n",
        "            para_text = \" \".join(text_segments).strip()\n",
        "            \n",
        "            # Get bounding box\n",
        "            bbox = get_bounding_box(paragraph.layout.bounding_poly)\n",
        "            \n",
        "            element = {\n",
        "                \"type\": \"paragraph\",\n",
        "                \"page\": page_num,\n",
        "                \"index\": para_idx,\n",
        "                \"text\": para_text,\n",
        "                \"bbox\": bbox,\n",
        "                \"char_count\": len(para_text)\n",
        "            }\n",
        "            \n",
        "            all_elements.append(element)\n",
        "            stats[\"paragraphs\"] += 1\n",
        "    \n",
        "    # Extract tables\n",
        "    for table_idx, table in enumerate(page.tables):\n",
        "        # Extract table data\n",
        "        table_data = extract_table_cells(table, document.text)\n",
        "        \n",
        "        bbox = get_bounding_box(table.layout.bounding_poly)\n",
        "        \n",
        "        element = {\n",
        "            \"type\": \"table\",\n",
        "            \"page\": page_num,\n",
        "            \"index\": table_idx,\n",
        "            \"rows\": len(table.body_rows) if hasattr(table, 'body_rows') else 0,\n",
        "            \"table_data\": table_data,\n",
        "            \"bbox\": bbox\n",
        "        }\n",
        "        \n",
        "        all_elements.append(element)\n",
        "        stats[\"tables\"] += 1\n",
        "    \n",
        "    # Extract images\n",
        "    if hasattr(page, 'image'):\n",
        "        for img_idx, image in enumerate(page.image):\n",
        "            bbox = get_bounding_box(image.layout.bounding_poly)\n",
        "            \n",
        "            element = {\n",
        "                \"type\": \"image\",\n",
        "                \"page\": page_num,\n",
        "                \"index\": img_idx,\n",
        "                \"bbox\": bbox,\n",
        "                \"area\": (bbox['x_max'] - bbox['x_min']) * (bbox['y_max'] - bbox['y_min'])\n",
        "            }\n",
        "            \n",
        "            all_elements.append(element)\n",
        "            stats[\"images\"] += 1\n",
        "\n",
        "print(\"‚úÖ Structure detection complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"‚úÖ STRUCTURE DETECTION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìä Statistics:\")\n",
        "for element_type, count in stats.items():\n",
        "    print(f\"  {element_type.capitalize()}: {count}\")\n",
        "\n",
        "print(f\"\\nTotal elements detected: {len(all_elements)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Visualize Sample Elements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show first paragraph\n",
        "paragraphs = [e for e in all_elements if e['type'] == 'paragraph']\n",
        "if paragraphs:\n",
        "    print(\"\\nüìÑ Sample Paragraph:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(paragraphs[0]['text'][:300])\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Show table info\n",
        "tables = [e for e in all_elements if e['type'] == 'table']\n",
        "if tables:\n",
        "    print(f\"\\nüìä Found {len(tables)} table(s)\")\n",
        "    print(f\"   First table: {tables[0]['rows']} rows, {len(tables[0]['table_data'])} cells\")\n",
        "\n",
        "# Show image info\n",
        "images = [e for e in all_elements if e['type'] == 'image']\n",
        "if images:\n",
        "    print(f\"\\nüñºÔ∏è  Found {len(images)} image(s)\")\n",
        "    for i, img in enumerate(images[:3]):\n",
        "        print(f\"   Image {i+1}: {img['area']*100:.1f}% of page\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"pdf_file\": Path(pdf_path).name,\n",
        "    \"total_pages\": len(document.pages),\n",
        "    \"statistics\": stats,\n",
        "    \"elements\": all_elements\n",
        "}\n",
        "\n",
        "output_path = \"output/test2_structured.json\"\n",
        "os.makedirs('output', exist_ok=True)\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"üíæ Results saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files.download(output_path)\n",
        "print(\"üì• Download started!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test 2: Structure Detection with Layout Parser\n",
        "\n",
        "**Goal:** Detect and analyze document structure (paragraphs, tables, images, headers)\n",
        "\n",
        "**What this test does:**\n",
        "- Uses Layout Parser to identify structural elements\n",
        "- Extracts paragraphs with bounding boxes\n",
        "- Detects tables and extracts table data\n",
        "- Identifies images and diagrams\n",
        "- Provides element-level statistics\n",
        "\n",
        "**Layout Parser Advantages:**\n",
        "- Superior structure detection compared to basic OCR\n",
        "- Accurate table boundary detection\n",
        "- Multi-column layout support\n",
        "- Better handling of complex document layouts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Steps (Run cells 2-8 same as Test 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q google-cloud-documentai python-dotenv openai anthropic pdf2image Pillow\n",
        "print(\"‚úÖ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload credentials\n",
        "from google.colab import files\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"üì§ Please upload your Google Cloud credentials JSON file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "creds_filename = list(uploaded.keys())[0]\n",
        "credentials_content = json.loads(uploaded[creds_filename].decode('utf-8'))\n",
        "\n",
        "with open('docai-credentials.json', 'w') as f:\n",
        "    json.dump(credentials_content, f)\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'docai-credentials.json'\n",
        "print(f\"‚úÖ Credentials saved: {creds_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure environment - UPDATE THESE VALUES\n",
        "DOCAI_PROJECT_ID = \"your-project-id-here\"\n",
        "DOCAI_PROCESSOR_ID = \"your-processor-id-here\"\n",
        "DOCAI_LOCATION = \"us\"\n",
        "\n",
        "os.environ['DOCAI_PROJECT_ID'] = DOCAI_PROJECT_ID\n",
        "os.environ['DOCAI_PROCESSOR_ID'] = DOCAI_PROCESSOR_ID\n",
        "os.environ['DOCAI_LOCATION'] = DOCAI_LOCATION\n",
        "\n",
        "print(f\"‚úÖ Configuration set for Layout Parser\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/abhii-01/docai-extraction-test.git\n",
        "%cd docai-extraction-test\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from utils.docai_client import get_client_from_env\n",
        "\n",
        "print(\"‚úÖ Repository cloned and utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify setup\n",
        "client = get_client_from_env()\n",
        "client.verify_setup()\n",
        "print(\"\\n‚úÖ Ready to detect document structure!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload PDF (preferably one with tables and images)\n",
        "print(\"üì§ Please upload your PDF file (with tables/structure)...\")\n",
        "uploaded_pdfs = files.upload()\n",
        "\n",
        "pdf_filename = list(uploaded_pdfs.keys())[0]\n",
        "pdf_path = pdf_filename\n",
        "\n",
        "print(f\"‚úÖ PDF uploaded: {pdf_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Document and Detect Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(\"TEST 2: STRUCTURE DETECTION\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Process document with Layout Parser\n",
        "print(f\"üìÑ Processing PDF with Layout Parser: {pdf_path}\")\n",
        "document = client.process_document(pdf_path)\n",
        "\n",
        "print(f\"‚úÖ Document processed!\")\n",
        "print(f\"   Total pages: {len(document.pages)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Structured Elements\n",
        "\n",
        "Define helper functions for extracting bounding boxes and table cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bounding_box(bounding_poly):\n",
        "    \"\"\"Extract normalized bounding box from polygon\"\"\"\n",
        "    if not bounding_poly or not hasattr(bounding_poly, 'normalized_vertices'):\n",
        "        return {\"x_min\": 0, \"y_min\": 0, \"x_max\": 0, \"y_max\": 0}\n",
        "    \n",
        "    vertices = bounding_poly.normalized_vertices\n",
        "    if not vertices:\n",
        "        return {\"x_min\": 0, \"y_min\": 0, \"x_max\": 0, \"y_max\": 0}\n",
        "    \n",
        "    x_coords = [v.x for v in vertices]\n",
        "    y_coords = [v.y for v in vertices]\n",
        "    \n",
        "    return {\n",
        "        \"x_min\": min(x_coords),\n",
        "        \"y_min\": min(y_coords),\n",
        "        \"x_max\": max(x_coords),\n",
        "        \"y_max\": max(y_coords)\n",
        "    }\n",
        "\n",
        "def extract_cell_text(cell, full_text):\n",
        "    \"\"\"Extract text from a table cell\"\"\"\n",
        "    if not hasattr(cell, 'layout') or not cell.layout.text_anchor:\n",
        "        return \"\"\n",
        "    \n",
        "    text_segments = []\n",
        "    for segment in cell.layout.text_anchor.text_segments:\n",
        "        text = full_text[segment.start_index:segment.end_index]\n",
        "        text_segments.append(text)\n",
        "    \n",
        "    return \" \".join(text_segments).strip()\n",
        "\n",
        "def extract_table_cells(table, full_text):\n",
        "    \"\"\"Extract table cells into structured format\"\"\"\n",
        "    cells = []\n",
        "    \n",
        "    # Extract header rows\n",
        "    if hasattr(table, 'header_rows'):\n",
        "        for row in table.header_rows:\n",
        "            for cell in row.cells:\n",
        "                text = extract_cell_text(cell, full_text)\n",
        "                cells.append({\n",
        "                    \"row\": cell.layout.table_row_index if hasattr(cell.layout, 'table_row_index') else 0,\n",
        "                    \"col\": cell.layout.table_col_index if hasattr(cell.layout, 'table_col_index') else 0,\n",
        "                    \"text\": text,\n",
        "                    \"is_header\": True\n",
        "                })\n",
        "    \n",
        "    # Extract body rows\n",
        "    if hasattr(table, 'body_rows'):\n",
        "        for row in table.body_rows:\n",
        "            for cell in row.cells:\n",
        "                text = extract_cell_text(cell, full_text)\n",
        "                cells.append({\n",
        "                    \"row\": cell.layout.table_row_index if hasattr(cell.layout, 'table_row_index') else 0,\n",
        "                    \"col\": cell.layout.table_col_index if hasattr(cell.layout, 'table_col_index') else 0,\n",
        "                    \"text\": text,\n",
        "                    \"is_header\": False\n",
        "                })\n",
        "    \n",
        "    return cells\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract All Structural Elements\n",
        "\n",
        "Extract paragraphs, tables, and images from all pages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç Detecting document structure...\\n\")\n",
        "\n",
        "all_elements = []\n",
        "stats = {\n",
        "    \"paragraphs\": 0,\n",
        "    \"tables\": 0,\n",
        "    \"images\": 0,\n",
        "    \"headers\": 0,\n",
        "    \"footers\": 0\n",
        "}\n",
        "\n",
        "for page_num, page in enumerate(document.pages, 1):\n",
        "    print(f\"  Processing page {page_num}...\")\n",
        "    \n",
        "    # Extract paragraphs\n",
        "    for para_idx, paragraph in enumerate(page.paragraphs):\n",
        "        if paragraph.layout.text_anchor:\n",
        "            # Get text\n",
        "            text_segments = []\n",
        "            for segment in paragraph.layout.text_anchor.text_segments:\n",
        "                text = document.text[segment.start_index:segment.end_index]\n",
        "                text_segments.append(text)\n",
        "            \n",
        "            para_text = \" \".join(text_segments).strip()\n",
        "            \n",
        "            # Get bounding box\n",
        "            bbox = get_bounding_box(paragraph.layout.bounding_poly)\n",
        "            \n",
        "            element = {\n",
        "                \"type\": \"paragraph\",\n",
        "                \"page\": page_num,\n",
        "                \"index\": para_idx,\n",
        "                \"text\": para_text,\n",
        "                \"bbox\": bbox,\n",
        "                \"char_count\": len(para_text)\n",
        "            }\n",
        "            \n",
        "            all_elements.append(element)\n",
        "            stats[\"paragraphs\"] += 1\n",
        "    \n",
        "    # Extract tables\n",
        "    for table_idx, table in enumerate(page.tables):\n",
        "        # Extract table data\n",
        "        table_data = extract_table_cells(table, document.text)\n",
        "        \n",
        "        bbox = get_bounding_box(table.layout.bounding_poly)\n",
        "        \n",
        "        element = {\n",
        "            \"type\": \"table\",\n",
        "            \"page\": page_num,\n",
        "            \"index\": table_idx,\n",
        "            \"rows\": len(table.body_rows) if hasattr(table, 'body_rows') else 0,\n",
        "            \"table_data\": table_data,\n",
        "            \"bbox\": bbox\n",
        "        }\n",
        "        \n",
        "        all_elements.append(element)\n",
        "        stats[\"tables\"] += 1\n",
        "    \n",
        "    # Extract images\n",
        "    if hasattr(page, 'image'):\n",
        "        for img_idx, image in enumerate(page.image):\n",
        "            bbox = get_bounding_box(image.layout.bounding_poly)\n",
        "            \n",
        "            element = {\n",
        "                \"type\": \"image\",\n",
        "                \"page\": page_num,\n",
        "                \"index\": img_idx,\n",
        "                \"bbox\": bbox,\n",
        "                \"area\": (bbox['x_max'] - bbox['x_min']) * (bbox['y_max'] - bbox['y_min'])\n",
        "            }\n",
        "            \n",
        "            all_elements.append(element)\n",
        "            stats[\"images\"] += 1\n",
        "\n",
        "print(f\"\\n‚úÖ Structure detection complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Statistics\n",
        "\n",
        "Show summary of detected elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(\"üìä STRUCTURE DETECTION STATISTICS\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "for element_type, count in stats.items():\n",
        "    print(f\"  {element_type.capitalize()}: {count}\")\n",
        "\n",
        "print(f\"\\n  Total elements: {len(all_elements)}\")\n",
        "print(f\"  Total pages: {len(document.pages)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preview Elements\n",
        "\n",
        "Show samples of detected elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show first paragraph\n",
        "paragraphs = [e for e in all_elements if e['type'] == 'paragraph']\n",
        "if paragraphs:\n",
        "    print(\"\\nüìÑ First Paragraph:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(paragraphs[0]['text'][:300])\n",
        "    if len(paragraphs[0]['text']) > 300:\n",
        "        print(\"...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Show table info\n",
        "tables = [e for e in all_elements if e['type'] == 'table']\n",
        "if tables:\n",
        "    print(f\"\\nüìä Tables Found: {len(tables)}\")\n",
        "    for i, table in enumerate(tables[:3], 1):\n",
        "        print(f\"  Table {i}: {table['rows']} rows, Page {table['page']}\")\n",
        "\n",
        "# Show image info\n",
        "images = [e for e in all_elements if e['type'] == 'image']\n",
        "if images:\n",
        "    print(f\"\\nüñºÔ∏è  Images Found: {len(images)}\")\n",
        "    for i, img in enumerate(images[:3], 1):\n",
        "        print(f\"  Image {i}: {img['area']:.2%} of page, Page {img['page']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build results\n",
        "results = {\n",
        "    \"pdf_file\": Path(pdf_path).name,\n",
        "    \"total_pages\": len(document.pages),\n",
        "    \"statistics\": stats,\n",
        "    \"elements\": all_elements\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "output_path = \"test2_structured.json\"\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nüíæ Results saved to: {output_path}\")\n",
        "\n",
        "# Download results\n",
        "files.download(output_path)\n",
        "print(f\"‚úÖ Test 2 complete! Results downloaded.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
