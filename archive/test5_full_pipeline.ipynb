{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test 5: Full Pipeline - Complete Textbook Processing\n",
        "\n",
        "**Goal:** Run end-to-end extraction pipeline matching your textbook taxonomy architecture\n",
        "\n",
        "This notebook combines **ALL features** for complete textbook processing with Layout Parser.\n",
        "\n",
        "## What This Test Does:\n",
        "- ‚úÖ Processes entire PDF with Layout Parser\n",
        "- ‚úÖ Extracts paragraphs as text chunks\n",
        "- ‚úÖ Detects and converts tables to narrative paragraphs\n",
        "- ‚úÖ Identifies flowcharts and generates descriptions\n",
        "- ‚úÖ Tags each chunk with structure type (paragraph/table/flowchart)\n",
        "- ‚úÖ Creates ready-to-use chunks for taxonomy matching\n",
        "\n",
        "**Complete Pipeline:**\n",
        "1. Layout Parser detection (paragraphs, tables, images)\n",
        "2. Table ‚Üí narrative conversion (LLM)\n",
        "3. Flowchart ‚Üí description (Vision LLM)\n",
        "4. Structure tagging for each chunk\n",
        "5. Export chunks ready for semantic search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q google-cloud-documentai python-dotenv openai anthropic pdf2image Pillow\n",
        "print(\"‚úÖ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"üì§ Please upload your Google Cloud credentials JSON file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "creds_filename = list(uploaded.keys())[0]\n",
        "credentials_content = json.loads(uploaded[creds_filename].decode('utf-8'))\n",
        "\n",
        "with open('docai-credentials.json', 'w') as f:\n",
        "    json.dump(credentials_content, f)\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'docai-credentials.json'\n",
        "print(f\"‚úÖ Credentials saved: {creds_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE VALUES\n",
        "DOCAI_PROJECT_ID = \"your-project-id-here\"\n",
        "DOCAI_PROCESSOR_ID = \"your-layout-parser-processor-id\"\n",
        "DOCAI_LOCATION = \"us\"\n",
        "\n",
        "# LLM Configuration\n",
        "OPENAI_API_KEY = \"sk-your-openai-key-here\"\n",
        "LLM_PROVIDER = \"openai\"\n",
        "LLM_MODEL = \"gpt-4o\"\n",
        "\n",
        "os.environ['DOCAI_PROJECT_ID'] = DOCAI_PROJECT_ID\n",
        "os.environ['DOCAI_PROCESSOR_ID'] = DOCAI_PROCESSOR_ID\n",
        "os.environ['DOCAI_LOCATION'] = DOCAI_LOCATION\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['LLM_PROVIDER'] = LLM_PROVIDER\n",
        "os.environ['LLM_MODEL'] = LLM_MODEL\n",
        "\n",
        "print(f\"‚úÖ Configuration set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Clone Repository and Load Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/abhii-01/python-automation.git\n",
        "%cd python-automation\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from utils.docai_client import get_client_from_env\n",
        "from utils.table_converter import table_to_markdown, table_to_narrative, detect_table_type\n",
        "from utils.vision_llm import describe_image_with_llm, extract_image_from_pdf, is_likely_diagram\n",
        "\n",
        "print(\"‚úÖ Repository cloned and utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç Verifying Document AI setup...\\n\")\n",
        "\n",
        "try:\n",
        "    client = get_client_from_env()\n",
        "    client.verify_setup()\n",
        "    print(\"\\n‚úÖ Setup verified! Ready to process documents.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Setup verification failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Upload PDF for Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì§ Please upload your PDF file...\")\n",
        "uploaded_pdfs = files.upload()\n",
        "\n",
        "pdf_filename = list(uploaded_pdfs.keys())[0]\n",
        "pdf_path = pdf_filename\n",
        "\n",
        "print(f\"‚úÖ PDF uploaded: {pdf_filename}\")\n",
        "print(f\"   Size: {len(uploaded_pdfs[pdf_filename]) / 1024:.1f} KB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Process Document with Layout Parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(\"TEST 5: FULL PIPELINE\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "print(f\"üìÑ Processing PDF with Layout Parser: {pdf_path}\")\n",
        "document = client.process_document(pdf_path)\n",
        "\n",
        "print(f\"‚úÖ Document processed!\")\n",
        "print(f\"   Total pages: {len(document.pages)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Define Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_table_simple(table, full_text):\n",
        "    \"\"\"Extract table into 2D list\"\"\"\n",
        "    cells_dict = {}\n",
        "    max_row = max_col = 0\n",
        "    \n",
        "    all_cells = []\n",
        "    if hasattr(table, 'header_rows'):\n",
        "        for row in table.header_rows:\n",
        "            all_cells.extend(row.cells)\n",
        "    if hasattr(table, 'body_rows'):\n",
        "        for row in table.body_rows:\n",
        "            all_cells.extend(row.cells)\n",
        "    \n",
        "    for cell in all_cells:\n",
        "        if not hasattr(cell, 'layout') or not cell.layout.text_anchor:\n",
        "            continue\n",
        "        \n",
        "        row_idx = getattr(cell.layout, 'table_row_index', 0)\n",
        "        col_idx = getattr(cell.layout, 'table_col_index', 0)\n",
        "        \n",
        "        text_parts = []\n",
        "        for segment in cell.layout.text_anchor.text_segments:\n",
        "            text = full_text[segment.start_index:segment.end_index]\n",
        "            text_parts.append(text)\n",
        "        \n",
        "        cells_dict[(row_idx, col_idx)] = \" \".join(text_parts).strip()\n",
        "        max_row = max(max_row, row_idx)\n",
        "        max_col = max(max_col, col_idx)\n",
        "    \n",
        "    if not cells_dict:\n",
        "        return []\n",
        "    \n",
        "    table_data = []\n",
        "    for row in range(max_row + 1):\n",
        "        row_data = [cells_dict.get((row, col), \"\") for col in range(max_col + 1)]\n",
        "        table_data.append(row_data)\n",
        "    \n",
        "    return table_data\n",
        "\n",
        "def get_bbox(bounding_poly):\n",
        "    \"\"\"Extract normalized bounding box\"\"\"\n",
        "    if not bounding_poly or not hasattr(bounding_poly, 'normalized_vertices'):\n",
        "        return {\"x_min\": 0, \"y_min\": 0, \"x_max\": 0, \"y_max\": 0}\n",
        "    \n",
        "    vertices = bounding_poly.normalized_vertices\n",
        "    if not vertices:\n",
        "        return {\"x_min\": 0, \"y_min\": 0, \"x_max\": 0, \"y_max\": 0}\n",
        "    \n",
        "    x_coords = [v.x for v in vertices]\n",
        "    y_coords = [v.y for v in vertices]\n",
        "    \n",
        "    return {\n",
        "        \"x_min\": min(x_coords),\n",
        "        \"y_min\": min(y_coords),\n",
        "        \"x_max\": max(x_coords),\n",
        "        \"y_max\": max(y_coords)\n",
        "    }\n",
        "\n",
        "def get_page_text(page, full_text):\n",
        "    \"\"\"Extract text from page\"\"\"\n",
        "    text_parts = []\n",
        "    for para in page.paragraphs:\n",
        "        if para.layout.text_anchor:\n",
        "            for segment in para.layout.text_anchor.text_segments:\n",
        "                text_parts.append(full_text[segment.start_index:segment.end_index])\n",
        "    return \" \".join(text_parts)\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüîÑ Running full extraction pipeline...\\n\")\n",
        "\n",
        "all_chunks = []\n",
        "chunk_id = 0\n",
        "\n",
        "stats = {\n",
        "    \"paragraphs\": 0,\n",
        "    \"tables\": 0,\n",
        "    \"flowcharts\": 0,\n",
        "    \"total_chunks\": 0\n",
        "}\n",
        "\n",
        "for page_num, page in enumerate(document.pages, 1):\n",
        "    print(f\"  üìÑ Page {page_num}\")\n",
        "    \n",
        "    # Extract paragraphs\n",
        "    for para in page.paragraphs:\n",
        "        if not para.layout.text_anchor:\n",
        "            continue\n",
        "        \n",
        "        text_parts = []\n",
        "        for segment in para.layout.text_anchor.text_segments:\n",
        "            text = document.text[segment.start_index:segment.end_index]\n",
        "            text_parts.append(text)\n",
        "        \n",
        "        para_text = \" \".join(text_parts).strip()\n",
        "        \n",
        "        if len(para_text) < 30:  # Skip short paragraphs\n",
        "            continue\n",
        "        \n",
        "        chunk_id += 1\n",
        "        chunk = {\n",
        "            \"chunk_id\": f\"chunk_{chunk_id:04d}\",\n",
        "            \"structure_type\": \"paragraph\",\n",
        "            \"page\": page_num,\n",
        "            \"text\": para_text,\n",
        "            \"char_count\": len(para_text),\n",
        "            \"metadata\": {\n",
        "                \"extraction_method\": \"layout_parser\"\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        all_chunks.append(chunk)\n",
        "        stats[\"paragraphs\"] += 1\n",
        "    \n",
        "    # Extract and convert tables\n",
        "    if hasattr(page, 'tables'):\n",
        "        for table_idx, table in enumerate(page.tables):\n",
        "            table_data = extract_table_simple(table, document.text)\n",
        "            \n",
        "            if not table_data:\n",
        "                continue\n",
        "            \n",
        "            markdown = table_to_markdown(table_data)\n",
        "            table_type = detect_table_type(table_data)\n",
        "            \n",
        "            try:\n",
        "                narrative = table_to_narrative(markdown, method=table_type)\n",
        "                print(f\"    ‚úÖ Converted table {table_idx + 1} to narrative\")\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ö†Ô∏è  Table conversion failed: {e}\")\n",
        "                narrative = f\"[Table with {len(table_data)} rows and {len(table_data[0])} columns]\"\n",
        "            \n",
        "            chunk_id += 1\n",
        "            chunk = {\n",
        "                \"chunk_id\": f\"chunk_{chunk_id:04d}\",\n",
        "                \"structure_type\": \"table\",\n",
        "                \"page\": page_num,\n",
        "                \"text\": narrative,\n",
        "                \"char_count\": len(narrative),\n",
        "                \"metadata\": {\n",
        "                    \"extraction_method\": \"layout_parser_table\",\n",
        "                    \"conversion_method\": \"llm_narrative\",\n",
        "                    \"table_type\": table_type,\n",
        "                    \"original_markdown\": markdown\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            all_chunks.append(chunk)\n",
        "            stats[\"tables\"] += 1\n",
        "    \n",
        "    # Extract and describe flowcharts/diagrams\n",
        "    if hasattr(page, 'image'):\n",
        "        for img_idx, image in enumerate(page.image):\n",
        "            bbox = get_bbox(image.layout.bounding_poly)\n",
        "            page_text = get_page_text(page, document.text)\n",
        "            \n",
        "            if not is_likely_diagram(bbox, page_text):\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                image_bytes = extract_image_from_pdf(pdf_path, page_num - 1, bbox)\n",
        "                description = describe_image_with_llm(image_bytes, image_type=\"flowchart\")\n",
        "                print(f\"    ‚úÖ Described diagram {img_idx + 1}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ö†Ô∏è  Diagram description failed: {e}\")\n",
        "                description = \"[Flowchart description unavailable]\"\n",
        "            \n",
        "            chunk_id += 1\n",
        "            chunk = {\n",
        "                \"chunk_id\": f\"chunk_{chunk_id:04d}\",\n",
        "                \"structure_type\": \"flowchart\",\n",
        "                \"page\": page_num,\n",
        "                \"text\": description,\n",
        "                \"char_count\": len(description),\n",
        "                \"metadata\": {\n",
        "                    \"extraction_method\": \"vision_llm\",\n",
        "                    \"bbox\": bbox\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            all_chunks.append(chunk)\n",
        "            stats[\"flowcharts\"] += 1\n",
        "\n",
        "stats[\"total_chunks\"] = len(all_chunks)\n",
        "\n",
        "print(f\"\\n‚úÖ Pipeline complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(\"‚úÖ FULL PIPELINE COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"üìä Statistics:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"  {key.replace('_', ' ').capitalize()}: {value}\")\n",
        "\n",
        "print(f\"\\nüí° These chunks are ready for taxonomy matching!\")\n",
        "print(f\"   Each chunk has structure_type tag and clean text.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: View Sample Chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show first chunk of each type\n",
        "for chunk_type in [\"paragraph\", \"table\", \"flowchart\"]:\n",
        "    chunks = [c for c in all_chunks if c['structure_type'] == chunk_type]\n",
        "    if chunks:\n",
        "        example = chunks[0]\n",
        "        print(f\"\\nüìù Sample {chunk_type.upper()}:\")\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"ID: {example['chunk_id']}\")\n",
        "        print(f\"Text: {example['text'][:200]}...\")\n",
        "        print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Save Results to JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"pdf_file\": Path(pdf_path).name,\n",
        "    \"total_pages\": len(document.pages),\n",
        "    \"statistics\": stats,\n",
        "    \"chunks\": all_chunks,\n",
        "    \"pipeline_config\": {\n",
        "        \"extraction_tool\": \"google_document_ai_layout_parser\",\n",
        "        \"table_conversion\": \"llm_narrative\",\n",
        "        \"flowchart_handling\": \"vision_llm\",\n",
        "        \"structure_tagging\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "output_path = \"test5_full_pipeline.json\"\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nüíæ Results saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files.download(output_path)\n",
        "print(f\"‚úÖ Test 5 complete! Results downloaded.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
