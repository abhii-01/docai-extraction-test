{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test 3: Table Handling with Layout Parser\n",
        "\n",
        "**Goal:** Extract tables and convert them to narrative paragraphs using LLM\n",
        "\n",
        "**What this test does:**\n",
        "- Detects tables using Layout Parser (superior table detection)\n",
        "- Extracts table data into structured format\n",
        "- Converts tables to markdown format\n",
        "- Uses LLM to generate narrative descriptions of tables\n",
        "- Saves both markdown and narrative versions\n",
        "\n",
        "**Why convert tables to narratives?**\n",
        "- Better for semantic search and taxonomy matching\n",
        "- Makes textbook content more accessible\n",
        "- Preserves table information in readable format\n",
        "- Layout Parser provides excellent table boundary detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Run this cell to install required packages for Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q google-cloud-documentai python-dotenv openai anthropic pdf2image Pillow\n",
        "print(\"âœ… All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Credentials\n",
        "\n",
        "Upload your Google Cloud service account JSON file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "creds_filename = list(uploaded.keys())[0]\n",
        "credentials_content = json.loads(uploaded[creds_filename].decode('utf-8'))\n",
        "\n",
        "with open('docai-credentials.json', 'w') as f:\n",
        "    json.dump(credentials_content, f)\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'docai-credentials.json'\n",
        "print(f\"âœ… Credentials saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Environment\n",
        "\n",
        "Set your Google Cloud project ID and Layout Parser processor ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE VALUES\n",
        "DOCAI_PROJECT_ID = \"your-project-id-here\"\n",
        "DOCAI_PROCESSOR_ID = \"your-processor-id-here\"\n",
        "DOCAI_LOCATION = \"us\"\n",
        "\n",
        "# LLM Configuration for table conversion\n",
        "OPENAI_API_KEY = \"sk-your-openai-key-here\"  # Or use ANTHROPIC_API_KEY\n",
        "LLM_PROVIDER = \"openai\"  # or \"anthropic\"\n",
        "LLM_MODEL = \"gpt-4o\"\n",
        "\n",
        "os.environ['DOCAI_PROJECT_ID'] = DOCAI_PROJECT_ID\n",
        "os.environ['DOCAI_PROCESSOR_ID'] = DOCAI_PROCESSOR_ID\n",
        "os.environ['DOCAI_LOCATION'] = DOCAI_LOCATION\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['LLM_PROVIDER'] = LLM_PROVIDER\n",
        "os.environ['LLM_MODEL'] = LLM_MODEL\n",
        "\n",
        "print(f\"âœ… Configuration set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Clone Repository and Load Utils\n",
        "\n",
        "Clone the repository to access utility functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/abhii-01/python-automation.git\n",
        "%cd python-automation\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from utils.docai_client import get_client_from_env\n",
        "from utils.table_converter import (\n",
        "    extract_table_data,\n",
        "    table_to_markdown,\n",
        "    table_to_narrative,\n",
        "    detect_table_type\n",
        ")\n",
        "\n",
        "print(\"âœ… Repository cloned and utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Setup\n",
        "\n",
        "Test connection to Google Document AI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ” Verifying Document AI setup...\\n\")\n",
        "\n",
        "try:\n",
        "    client = get_client_from_env()\n",
        "    client.verify_setup()\n",
        "    print(\"\\nâœ… Setup verified! Ready to process documents.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Setup verification failed: {e}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"  1. Credentials file is valid\")\n",
        "    print(\"  2. Project ID is correct\")\n",
        "    print(\"  3. Processor ID is correct\")\n",
        "    print(\"  4. Layout Parser processor exists and is enabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Upload PDF for Testing\n",
        "\n",
        "Upload your PDF file with tables to process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ“¤ Please upload your PDF file (should contain tables)...\")\n",
        "uploaded_pdfs = files.upload()\n",
        "\n",
        "pdf_filename = list(uploaded_pdfs.keys())[0]\n",
        "pdf_path = pdf_filename\n",
        "\n",
        "print(f\"âœ… PDF uploaded: {pdf_filename}\")\n",
        "print(f\"   Size: {len(uploaded_pdfs[pdf_filename]) / 1024:.1f} KB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Process Document with Layout Parser\n",
        "\n",
        "Process the PDF to detect tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(\"TEST 3: TABLE HANDLING\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "print(f\"ğŸ“„ Processing PDF with Layout Parser: {pdf_path}\")\n",
        "document = client.process_document(pdf_path)\n",
        "\n",
        "print(f\"âœ… Document processed!\")\n",
        "print(f\"   Total pages: {len(document.pages)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Define Helper Function for Table Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = get_client_from_env()\n",
        "client.verify_setup()\n",
        "print(\"\\nâœ… Ready to process tables!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ“¤ Please upload a PDF with tables...\")\n",
        "uploaded_pdfs = files.upload()\n",
        "\n",
        "pdf_filename = list(uploaded_pdfs.keys())[0]\n",
        "pdf_path = pdf_filename\n",
        "\n",
        "print(f\"âœ… PDF uploaded: {pdf_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Document and Extract Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(\"TEST 3: TABLE HANDLING\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "print(f\"ğŸ“„ Processing PDF with Layout Parser: {pdf_path}\")\n",
        "document = client.process_document(pdf_path)\n",
        "\n",
        "print(f\"âœ… Document processed!\")\n",
        "print(f\"   Total pages: {len(document.pages)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Helper Function for Table Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_simple_table(table, full_text):\n",
        "    \"\"\"\n",
        "    Simplified table extraction\n",
        "    Returns 2D list of cell values\n",
        "    \"\"\"\n",
        "    cells_dict = {}\n",
        "    max_row = 0\n",
        "    max_col = 0\n",
        "    \n",
        "    # Extract all cells\n",
        "    all_cells = []\n",
        "    if hasattr(table, 'header_rows'):\n",
        "        for row in table.header_rows:\n",
        "            all_cells.extend(row.cells)\n",
        "    if hasattr(table, 'body_rows'):\n",
        "        for row in table.body_rows:\n",
        "            all_cells.extend(row.cells)\n",
        "    \n",
        "    # Build cell grid\n",
        "    for cell in all_cells:\n",
        "        if not hasattr(cell, 'layout') or not cell.layout.text_anchor:\n",
        "            continue\n",
        "        \n",
        "        # Get cell position\n",
        "        row_idx = cell.layout.table_row_index if hasattr(cell.layout, 'table_row_index') else 0\n",
        "        col_idx = cell.layout.table_col_index if hasattr(cell.layout, 'table_col_index') else 0\n",
        "        \n",
        "        # Get cell text\n",
        "        text_parts = []\n",
        "        for segment in cell.layout.text_anchor.text_segments:\n",
        "            text = full_text[segment.start_index:segment.end_index]\n",
        "            text_parts.append(text)\n",
        "        \n",
        "        cell_text = \" \".join(text_parts).strip()\n",
        "        \n",
        "        cells_dict[(row_idx, col_idx)] = cell_text\n",
        "        max_row = max(max_row, row_idx)\n",
        "        max_col = max(max_col, col_idx)\n",
        "    \n",
        "    # Convert to 2D list\n",
        "    if not cells_dict:\n",
        "        return []\n",
        "    \n",
        "    table_data = []\n",
        "    for row in range(max_row + 1):\n",
        "        row_data = []\n",
        "        for col in range(max_col + 1):\n",
        "            cell_text = cells_dict.get((row, col), \"\")\n",
        "            row_data.append(cell_text)\n",
        "        table_data.append(row_data)\n",
        "    \n",
        "    return table_data\n",
        "\n",
        "print(\"âœ… Helper function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract and Process Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nğŸ“Š Processing tables...\\n\")\n",
        "\n",
        "table_results = []\n",
        "table_count = 0\n",
        "\n",
        "for page_num, page in enumerate(document.pages, 1):\n",
        "    if not hasattr(page, 'tables') or not page.tables:\n",
        "        continue\n",
        "    \n",
        "    print(f\"  Page {page_num}: Found {len(page.tables)} table(s)\")\n",
        "    \n",
        "    for table_idx, table in enumerate(page.tables):\n",
        "        table_count += 1\n",
        "        print(f\"\\n  Processing table {table_count}...\")\n",
        "        \n",
        "        # Extract table data\n",
        "        table_data = extract_simple_table(table, document.text)\n",
        "        \n",
        "        if not table_data:\n",
        "            print(f\"    âš ï¸  Could not extract table data\")\n",
        "            continue\n",
        "        \n",
        "        # Convert to markdown\n",
        "        markdown = table_to_markdown(table_data)\n",
        "        print(f\"    âœ… Converted to markdown ({len(table_data)} rows)\")\n",
        "        \n",
        "        # Detect table type\n",
        "        table_type = detect_table_type(table_data)\n",
        "        print(f\"    ğŸ” Detected type: {table_type}\")\n",
        "        \n",
        "        # Convert to narrative\n",
        "        print(f\"    ğŸ¤– Converting to narrative paragraph...\")\n",
        "        try:\n",
        "            narrative = table_to_narrative(markdown, method=table_type)\n",
        "            print(f\"    âœ… Generated narrative ({len(narrative)} chars)\")\n",
        "        except Exception as e:\n",
        "            print(f\"    âš ï¸  LLM conversion failed: {e}\")\n",
        "            narrative = f\"[Table conversion failed: {e}]\"\n",
        "        \n",
        "        # Store result\n",
        "        result = {\n",
        "            \"table_id\": f\"table_{table_count}\",\n",
        "            \"page\": page_num,\n",
        "            \"table_index\": table_idx,\n",
        "            \"rows\": len(table_data),\n",
        "            \"columns\": len(table_data[0]) if table_data else 0,\n",
        "            \"detected_type\": table_type,\n",
        "            \"markdown\": markdown,\n",
        "            \"narrative\": narrative,\n",
        "            \"original_data\": table_data\n",
        "        }\n",
        "        \n",
        "        table_results.append(result)\n",
        "\n",
        "print(f\"\\nâœ… Processed {table_count} tables\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if table_results:\n",
        "    example = table_results[0]\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXAMPLE: Table 1 (Page {example['page']})\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    print(f\"ğŸ“Š Original Markdown:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(example['markdown'][:500])\n",
        "    if len(example['markdown']) > 500:\n",
        "        print(\"...\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    print(f\"\\nğŸ“ Generated Narrative:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(example['narrative'])\n",
        "    print(\"-\" * 60)\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No tables found in this document\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build final results\n",
        "results = {\n",
        "    \"pdf_file\": Path(pdf_path).name,\n",
        "    \"total_tables\": table_count,\n",
        "    \"tables\": table_results\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "output_path = \"test3_tables.json\"\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nğŸ’¾ Results saved to: {output_path}\")\n",
        "print(f\"\\nğŸ“Š Summary:\")\n",
        "print(f\"  Total tables found: {table_count}\")\n",
        "print(f\"  Successfully converted: {len(table_results)}\")\n",
        "\n",
        "# Download results\n",
        "files.download(output_path)\n",
        "print(f\"\\nâœ… Test 3 complete! Results downloaded.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
