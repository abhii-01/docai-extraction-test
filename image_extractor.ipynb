{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Extractor - Standalone Pipeline\n",
        "\n",
        "Extract images, charts, and diagrams from PDFs using Vision LLM.\n",
        "\n",
        "**Features:**\n",
        "- Uses `pdf2image` (FREE) to render PDF pages\n",
        "- Sends to Vision LLM to detect image regions\n",
        "- Crops and saves detected images\n",
        "- Generates descriptions for each image\n",
        "- Outputs `image_extractions.json`\n",
        "\n",
        "**No Google Doc AI / Layout Parser dependencies.**\n",
        "\n",
        "---\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Render**: Convert each PDF page to an image using Poppler\n",
        "2. **Detect**: Send page to Vision LLM â†’ get bounding boxes of images/charts\n",
        "3. **Crop**: Extract detected regions as separate image files\n",
        "4. **Describe**: Get text descriptions for each detected image\n",
        "5. **Output**: Save JSON with image metadata + descriptions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Python packages\n",
        "%pip install -q pdf2image Pillow openai\n",
        "\n",
        "# Install Poppler (required by pdf2image to render PDFs)\n",
        "!apt-get update -qq && apt-get install -y -qq poppler-utils\n",
        "\n",
        "print(\"âœ… Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION - Update these values\n",
        "# ============================================================\n",
        "\n",
        "# OpenAI API Key (for Vision LLM)\n",
        "OPENAI_API_KEY = \"\"  # Paste your key here, or set via environment\n",
        "\n",
        "# Output directory for extracted images\n",
        "OUTPUT_DIR = \"image_output\"\n",
        "\n",
        "# ============================================================\n",
        "# Set environment variable\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "elif not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"âš ï¸ Warning: OPENAI_API_KEY not set. Please set it above or via environment.\")\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"images\"), exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Output directory: {OUTPUT_DIR}/\")\n",
        "print(f\"âœ… Images will be saved to: {OUTPUT_DIR}/images/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Upload PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Upload a PDF file to extract images from...\")\n",
        "uploaded = files.upload()\n",
        "pdf_filename = list(uploaded.keys())[0]\n",
        "print(f\"âœ… Uploaded: {pdf_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Image Extractor Class\n",
        "\n",
        "Core logic for detecting and extracting images from PDF pages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import base64\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import io\n",
        "from openai import OpenAI\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class ImageExtractor:\n",
        "    \"\"\"\n",
        "    Extracts images, charts, and diagrams from PDFs using Vision LLM.\n",
        "    \n",
        "    Pipeline:\n",
        "    1. Render PDF pages as images (pdf2image + Poppler)\n",
        "    2. Send each page to Vision LLM to detect image regions\n",
        "    3. Crop detected regions and save to disk\n",
        "    4. Return structured JSON with image metadata + descriptions\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: str = \"image_output\", api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            output_dir: Directory to save extracted images\n",
        "            api_key: OpenAI API key (uses env var if not provided)\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        self.images_dir = os.path.join(output_dir, \"images\")\n",
        "        \n",
        "        # Create directories\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.images_dir, exist_ok=True)\n",
        "        \n",
        "        # Initialize OpenAI client\n",
        "        api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"OpenAI API key required. Set OPENAI_API_KEY or pass api_key parameter.\")\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        \n",
        "        # Counter for image IDs\n",
        "        self.image_counter = 0\n",
        "    \n",
        "    def extract(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Main entry point - extract all images from a PDF.\n",
        "        \n",
        "        Args:\n",
        "            pdf_path: Path to the PDF file\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with metadata and list of extracted images\n",
        "        \"\"\"\n",
        "        print(f\"ðŸ“„ Processing: {pdf_path}\")\n",
        "        \n",
        "        # 1. Render PDF pages as images\n",
        "        print(\"ðŸ”„ Rendering PDF pages...\")\n",
        "        page_images = self._render_pages(pdf_path)\n",
        "        print(f\"   âœ… Rendered {len(page_images)} pages\")\n",
        "        \n",
        "        # 2. Process each page to detect images\n",
        "        all_images = []\n",
        "        for page_num, page_image in enumerate(page_images, start=1):\n",
        "            print(f\"ðŸ” Analyzing page {page_num}/{len(page_images)}...\")\n",
        "            detected = self._detect_images_on_page(page_image, page_num)\n",
        "            all_images.extend(detected)\n",
        "            print(f\"   Found {len(detected)} image(s) on page {page_num}\")\n",
        "        \n",
        "        # 3. Build result\n",
        "        result = {\n",
        "            \"metadata\": {\n",
        "                \"source_file\": os.path.basename(pdf_path),\n",
        "                \"page_count\": len(page_images),\n",
        "                \"images_found\": len(all_images),\n",
        "                \"extraction_timestamp\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"images\": all_images\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nâœ… Extraction complete! Found {len(all_images)} images across {len(page_images)} pages.\")\n",
        "        return result\n",
        "    \n",
        "    def _render_pages(self, pdf_path: str) -> List[Image.Image]:\n",
        "        \"\"\"Convert PDF pages to PIL Images using pdf2image.\"\"\"\n",
        "        try:\n",
        "            images = convert_from_path(pdf_path, dpi=150)\n",
        "            return images\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error rendering PDF: {e}\")\n",
        "            print(\"   Make sure Poppler is installed: apt-get install -y poppler-utils\")\n",
        "            raise\n",
        "    \n",
        "    def _detect_images_on_page(self, page_image: Image.Image, page_num: int) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Send page to Vision LLM to detect image regions and get descriptions.\n",
        "        \n",
        "        Returns list of detected images with bounding boxes and descriptions.\n",
        "        \"\"\"\n",
        "        # Convert PIL Image to base64\n",
        "        img_bytes = io.BytesIO()\n",
        "        page_image.save(img_bytes, format='PNG')\n",
        "        img_bytes.seek(0)\n",
        "        base64_image = base64.b64encode(img_bytes.read()).decode('utf-8')\n",
        "        \n",
        "        # Build detection prompt\n",
        "        prompt = \"\"\"Analyze this PDF page and identify ALL visual elements that are NOT plain text.\n",
        "\n",
        "Look for:\n",
        "- Charts (bar, line, pie, etc.)\n",
        "- Diagrams and flowcharts\n",
        "- Infographics\n",
        "- Photographs or illustrations\n",
        "- Tables with visual formatting\n",
        "- Icons or logos (if significant)\n",
        "- QR codes\n",
        "\n",
        "For EACH visual element found, provide:\n",
        "1. bbox: Bounding box as [x_min, y_min, x_max, y_max] where values are percentages (0.0 to 1.0) of page width/height\n",
        "2. type: One of \"chart\", \"diagram\", \"infographic\", \"photo\", \"table\", \"icon\", \"qr_code\", \"other\"\n",
        "3. description: Brief description of what the visual shows (1-2 sentences)\n",
        "\n",
        "Respond in this exact JSON format:\n",
        "{\n",
        "  \"visuals\": [\n",
        "    {\n",
        "      \"bbox\": [0.1, 0.2, 0.5, 0.6],\n",
        "      \"type\": \"chart\",\n",
        "      \"description\": \"Bar chart showing quarterly revenue growth from Q1-Q4 2024\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "If NO visual elements are found, respond with:\n",
        "{\"visuals\": []}\n",
        "\n",
        "IMPORTANT: Only include actual images/visuals. Do NOT include plain text paragraphs or headings.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"text\", \"text\": prompt},\n",
        "                            {\n",
        "                                \"type\": \"image_url\",\n",
        "                                \"image_url\": {\n",
        "                                    \"url\": f\"data:image/png;base64,{base64_image}\",\n",
        "                                    \"detail\": \"high\"\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=1000,\n",
        "                temperature=0.1\n",
        "            )\n",
        "            \n",
        "            # Parse response\n",
        "            content = response.choices[0].message.content.strip()\n",
        "            \n",
        "            # Extract JSON from response (handle markdown code blocks)\n",
        "            json_match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
        "            if json_match:\n",
        "                parsed = json.loads(json_match.group())\n",
        "                visuals = parsed.get(\"visuals\", [])\n",
        "            else:\n",
        "                print(f\"   âš ï¸ Could not parse response: {content[:100]}...\")\n",
        "                visuals = []\n",
        "            \n",
        "            # Process each detected visual\n",
        "            detected_images = []\n",
        "            for visual in visuals:\n",
        "                bbox = visual.get(\"bbox\", [])\n",
        "                if len(bbox) != 4:\n",
        "                    continue\n",
        "                \n",
        "                # Crop and save the image\n",
        "                self.image_counter += 1\n",
        "                image_id = f\"img_{self.image_counter:03d}\"\n",
        "                file_path = self._crop_and_save(page_image, bbox, page_num, image_id)\n",
        "                \n",
        "                detected_images.append({\n",
        "                    \"id\": image_id,\n",
        "                    \"page\": page_num,\n",
        "                    \"type\": visual.get(\"type\", \"unknown\"),\n",
        "                    \"bbox\": bbox,\n",
        "                    \"file_path\": file_path,\n",
        "                    \"description\": visual.get(\"description\", \"\")\n",
        "                })\n",
        "            \n",
        "            return detected_images\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Error detecting images on page {page_num}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def _crop_and_save(self, page_image: Image.Image, bbox: List[float], \n",
        "                       page_num: int, image_id: str) -> str:\n",
        "        \"\"\"\n",
        "        Crop the detected region from the page image and save to disk.\n",
        "        \n",
        "        Args:\n",
        "            page_image: Full page as PIL Image\n",
        "            bbox: [x_min, y_min, x_max, y_max] as percentages (0.0-1.0)\n",
        "            page_num: Page number\n",
        "            image_id: Unique image identifier\n",
        "            \n",
        "        Returns:\n",
        "            Path to saved image file\n",
        "        \"\"\"\n",
        "        width, height = page_image.size\n",
        "        \n",
        "        # Convert normalized coordinates to pixels\n",
        "        x_min = int(bbox[0] * width)\n",
        "        y_min = int(bbox[1] * height)\n",
        "        x_max = int(bbox[2] * width)\n",
        "        y_max = int(bbox[3] * height)\n",
        "        \n",
        "        # Ensure valid crop box\n",
        "        x_min = max(0, x_min)\n",
        "        y_min = max(0, y_min)\n",
        "        x_max = min(width, x_max)\n",
        "        y_max = min(height, y_max)\n",
        "        \n",
        "        if x_max <= x_min or y_max <= y_min:\n",
        "            print(f\"   âš ï¸ Invalid bbox for {image_id}, skipping crop\")\n",
        "            return \"\"\n",
        "        \n",
        "        # Crop\n",
        "        cropped = page_image.crop((x_min, y_min, x_max, y_max))\n",
        "        \n",
        "        # Save\n",
        "        filename = f\"page{page_num}_{image_id}.png\"\n",
        "        save_path = os.path.join(self.images_dir, filename)\n",
        "        cropped.save(save_path, format='PNG')\n",
        "        \n",
        "        return save_path\n",
        "\n",
        "\n",
        "print(\"âœ… ImageExtractor class defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize extractor and run\n",
        "extractor = ImageExtractor(output_dir=OUTPUT_DIR)\n",
        "result = extractor.extract(pdf_filename)\n",
        "\n",
        "# Preview result summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EXTRACTION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Source: {result['metadata']['source_file']}\")\n",
        "print(f\"Pages: {result['metadata']['page_count']}\")\n",
        "print(f\"Images Found: {result['metadata']['images_found']}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Preview Extracted Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "if result[\"images\"]:\n",
        "    print(f\"Showing {len(result['images'])} extracted images:\\n\")\n",
        "    \n",
        "    for img_data in result[\"images\"]:\n",
        "        print(f\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
        "        print(f\"ðŸ“¸ {img_data['id']} | Page {img_data['page']} | Type: {img_data['type']}\")\n",
        "        print(f\"ðŸ“ {img_data['description']}\")\n",
        "        print(f\"ðŸ“ {img_data['file_path']}\")\n",
        "        \n",
        "        # Display the image\n",
        "        if img_data['file_path'] and os.path.exists(img_data['file_path']):\n",
        "            display(IPImage(filename=img_data['file_path'], width=400))\n",
        "        print()\n",
        "else:\n",
        "    print(\"No images were detected in this PDF.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save JSON Output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save JSON output\n",
        "output_json_path = os.path.join(OUTPUT_DIR, \"image_extractions.json\")\n",
        "\n",
        "with open(output_json_path, \"w\") as f:\n",
        "    json.dump(result, f, indent=2)\n",
        "\n",
        "print(f\"âœ… JSON saved to: {output_json_path}\")\n",
        "\n",
        "# Display JSON content\n",
        "print(\"\\nðŸ“„ JSON Content Preview:\")\n",
        "print(\"-\" * 50)\n",
        "print(json.dumps(result, indent=2)[:2000])\n",
        "if len(json.dumps(result, indent=2)) > 2000:\n",
        "    print(\"\\n... (truncated)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download JSON\n",
        "files.download(output_json_path)\n",
        "\n",
        "# Zip and download images if any were extracted\n",
        "if result[\"images\"]:\n",
        "    import shutil\n",
        "    zip_path = os.path.join(OUTPUT_DIR, \"extracted_images\")\n",
        "    shutil.make_archive(zip_path, 'zip', extractor.images_dir)\n",
        "    print(f\"âœ… Images zipped to: {zip_path}.zip\")\n",
        "    files.download(f\"{zip_path}.zip\")\n",
        "else:\n",
        "    print(\"No images to download.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
