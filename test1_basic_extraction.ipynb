{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test 1: Basic Text Extraction with Layout Parser\n",
        "\n",
        "**Goal:** Verify Google Document AI Layout Parser works and extracts raw text from PDFs\n",
        "\n",
        "**What this test does:**\n",
        "- Connects to Google Document AI with Layout Parser processor\n",
        "- Processes a PDF document\n",
        "- Extracts text content page by page\n",
        "- Saves results to JSON file\n",
        "\n",
        "**Layout Parser Benefits:**\n",
        "- Better structure detection than basic OCR\n",
        "- Preserves document layout hierarchy\n",
        "- Detects paragraphs, headers, and text blocks\n",
        "- Ideal for academic textbooks and complex documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Run this cell to install required packages for Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.13.7)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/Users/aadarsh/Documents/code/docai-test/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%pip install -q google-cloud-documentai python-dotenv openai anthropic pdf2image Pillow\n",
        "print(\"âœ… All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Credentials\n",
        "\n",
        "Upload your Google Cloud service account JSON file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"ðŸ“¤ Please upload your Google Cloud credentials JSON file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Save credentials\n",
        "creds_filename = list(uploaded.keys())[0]\n",
        "credentials_content = json.loads(uploaded[creds_filename].decode('utf-8'))\n",
        "\n",
        "# Write to temporary file\n",
        "with open('docai-credentials.json', 'w') as f:\n",
        "    json.dump(credentials_content, f)\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'docai-credentials.json'\n",
        "print(f\"âœ… Credentials saved: {creds_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Environment\n",
        "\n",
        "Set your Google Cloud project ID and Layout Parser processor ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE VALUES\n",
        "DOCAI_PROJECT_ID = \"your-project-id-here\"  # Replace with your project ID\n",
        "DOCAI_PROCESSOR_ID = \"your-processor-id-here\"  # Replace with your Layout Parser processor ID\n",
        "DOCAI_LOCATION = \"us\"  # us, eu, or asia\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['DOCAI_PROJECT_ID'] = DOCAI_PROJECT_ID\n",
        "os.environ['DOCAI_PROCESSOR_ID'] = DOCAI_PROCESSOR_ID\n",
        "os.environ['DOCAI_LOCATION'] = DOCAI_LOCATION\n",
        "\n",
        "print(f\"âœ… Configuration set:\")\n",
        "print(f\"   Project: {DOCAI_PROJECT_ID}\")\n",
        "print(f\"   Processor: {DOCAI_PROCESSOR_ID}\")\n",
        "print(f\"   Location: {DOCAI_LOCATION}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Clone Repository and Load Utils\n",
        "\n",
        "Clone the repository to access utility functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/abhii-01/docai-extraction-test.git\n",
        "%cd docai-extraction-test\n",
        "\n",
        "# Import utilities\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from utils.docai_client import get_client_from_env\n",
        "\n",
        "print(\"âœ… Repository cloned and utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Setup\n",
        "\n",
        "Test connection to Google Document AI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ” Verifying Document AI setup...\\n\")\n",
        "\n",
        "try:\n",
        "    client = get_client_from_env()\n",
        "    client.verify_setup()\n",
        "    print(\"\\nâœ… Setup verified! Ready to process documents.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Setup verification failed: {e}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"  1. Credentials file is valid\")\n",
        "    print(\"  2. Project ID is correct\")\n",
        "    print(\"  3. Processor ID is correct\")\n",
        "    print(\"  4. Layout Parser processor exists and is enabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Upload PDF for Testing\n",
        "\n",
        "Upload your PDF file to process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“¤ Please upload your PDF file...\")\n",
        "uploaded_pdfs = files.upload()\n",
        "\n",
        "# Get the first uploaded PDF\n",
        "pdf_filename = list(uploaded_pdfs.keys())[0]\n",
        "pdf_path = pdf_filename\n",
        "\n",
        "print(f\"âœ… PDF uploaded: {pdf_filename}\")\n",
        "print(f\"   Size: {len(uploaded_pdfs[pdf_filename]) / 1024:.1f} KB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Process Document with Layout Parser\n",
        "\n",
        "Extract text from the PDF using Layout Parser processor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(\"TEST 1: BASIC TEXT EXTRACTION\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Process document\n",
        "print(f\"ðŸ“„ Processing PDF with Layout Parser: {pdf_path}\")\n",
        "document = client.process_document(pdf_path)\n",
        "\n",
        "# Extract full text\n",
        "full_text = document.text\n",
        "print(f\"âœ… Document processed successfully!\")\n",
        "print(f\"   Total pages: {len(document.pages)}\")\n",
        "print(f\"   Total characters: {len(full_text):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Extract Page-by-Page Text\n",
        "\n",
        "Extract text from each page individually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“ Extracting text by page...\\n\")\n",
        "\n",
        "pages_data = []\n",
        "\n",
        "for page_num, page in enumerate(document.pages, 1):\n",
        "    page_text = \"\"\n",
        "    \n",
        "    # Get text from paragraphs\n",
        "    for paragraph in page.paragraphs:\n",
        "        if paragraph.layout.text_anchor:\n",
        "            # Extract text segments\n",
        "            text_segments = []\n",
        "            for segment in paragraph.layout.text_anchor.text_segments:\n",
        "                start = segment.start_index\n",
        "                end = segment.end_index\n",
        "                text_segments.append(document.text[start:end])\n",
        "            \n",
        "            page_text += \" \".join(text_segments) + \"\\n\\n\"\n",
        "    \n",
        "    pages_data.append({\n",
        "        \"page_number\": page_num,\n",
        "        \"text\": page_text.strip(),\n",
        "        \"char_count\": len(page_text)\n",
        "    })\n",
        "    \n",
        "    print(f\"  Page {page_num}: {len(page_text):,} characters\")\n",
        "\n",
        "print(f\"\\nâœ… Extracted text from {len(pages_data)} pages\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Preview Extracted Text\n",
        "\n",
        "Display a preview of the extracted text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“„ TEXT PREVIEW (first 800 characters):\")\n",
        "print(\"=\" * 60)\n",
        "print(full_text[:800])\n",
        "if len(full_text) > 800:\n",
        "    print(\"...\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Save Results to JSON\n",
        "\n",
        "Save extraction results to a JSON file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Build results\n",
        "results = {\n",
        "    \"pdf_file\": Path(pdf_path).name,\n",
        "    \"total_pages\": len(document.pages),\n",
        "    \"total_characters\": len(full_text),\n",
        "    \"full_text\": full_text,\n",
        "    \"pages\": pages_data\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "output_path = \"test1_raw_text.json\"\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"ðŸ’¾ Results saved to: {output_path}\")\n",
        "print(f\"\\nðŸ“Š Summary:\")\n",
        "print(f\"  Total pages: {len(document.pages)}\")\n",
        "print(f\"  Total characters: {len(full_text):,}\")\n",
        "print(f\"  Average chars/page: {len(full_text) // len(document.pages):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Download Results\n",
        "\n",
        "Download the JSON results file to your computer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"ðŸ“¥ Downloading results...\")\n",
        "files.download(output_path)\n",
        "print(f\"âœ… Test 1 complete! Results downloaded.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
